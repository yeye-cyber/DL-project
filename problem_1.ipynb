{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2wy2T+PtrUed7QRlnoF0r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeye-cyber/DL-project/blob/master/problem_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kA0POWp7vAZr",
        "outputId": "a0a8bf49-c716-40d6-a3e7-c941c12859e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: box2d in /usr/local/lib/python3.8/dist-packages (2.3.10)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install gym[box2d]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch.nn.modules.linear import Linear\n",
        "from torch.nn.modules.activation import ReLU\n",
        "import numpy as np\n",
        "import gym\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import trange\n",
        "from collections import deque, namedtuple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "class Agent(object):\n",
        "    ''' Base agent class, used as a parent class\n",
        "\n",
        "        Args:\n",
        "            n_actions (int): number of actions\n",
        "\n",
        "        Attributes:\n",
        "            n_actions (int): where we store the number of actions\n",
        "            last_action (int): last action taken by the agent\n",
        "    '''\n",
        "    def __init__(self, n_actions: int):\n",
        "        self.n_actions = n_actions\n",
        "        self.last_action = None\n",
        "\n",
        "    def forward(self, state: np.ndarray):\n",
        "        ''' Performs a forward computation '''\n",
        "        pass\n",
        "\n",
        "    def backward(self):\n",
        "        ''' Performs a backward pass on the network '''\n",
        "        pass\n",
        "\n",
        "\n",
        "class RandomAgent(Agent):\n",
        "    ''' Agent taking actions uniformly at random, child of the class Agent'''\n",
        "    def __init__(self, n_actions: int):\n",
        "        super(RandomAgent, self).__init__(n_actions)\n",
        "\n",
        "    def forward(self, state: np.ndarray) -> int:\n",
        "        ''' Compute an action uniformly at random across n_actions possible\n",
        "            choices\n",
        "\n",
        "            Returns:\n",
        "                action (int): the random action\n",
        "        '''\n",
        "        self.last_action = np.random.randint(0, self.n_actions)\n",
        "        return self.last_action\n",
        "\n",
        "\n",
        "class DQN(nn.Module):\n",
        "      def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, output_size)\n",
        "        )\n",
        "      def forward(self, state):\n",
        "        ''' Performs a forward computation '''\n",
        "        return self.network(state)\n",
        "        \n",
        "   \n",
        "class DQNAgent(object):\n",
        "    ''' Base agent class, used as a parent class\n",
        "\n",
        "        Args:\n",
        "            n_actions (int): number of actions\n",
        "\n",
        "        Attributes:\n",
        "            n_actions (int): where we store the number of actions\n",
        "            last_action (int): last action taken by the agent\n",
        "    '''\n",
        "    def __init__(self, n_actions: int):\n",
        "        self.n_actions = n_actions\n",
        "        self.last_action = None\n",
        "\n",
        "    def forward(self, state: np.ndarray):\n",
        "        ''' Performs a forward computation '''\n",
        "        pass\n",
        "\n",
        "    def backward(self):\n",
        "        ''' Performs a backward pass on the network '''\n",
        "        pass\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "Experience = namedtuple('Experience',\n",
        "                        ['state', 'action', 'reward', 'next_state', 'done'])\n",
        "\n",
        "class ExperienceReplayBuffer(object):\n",
        "    \"\"\" Class used to store a buffer containing experiences of the RL agent.\n",
        "    \"\"\"\n",
        "    def __init__(self, maximum_length):\n",
        "        # Create buffer of maximum length\n",
        "        self.buffer = deque(maxlen=maximum_length)\n",
        "\n",
        "    def append(self, experience):\n",
        "        # Append experience to the buffer\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def __len__(self):\n",
        "        # overload len operator\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def sample_batch(self, n):\n",
        "        \"\"\" Function used to sample experiences from the buffer.\n",
        "            returns 5 lists, each of size n. Returns a list of state, actions,\n",
        "            rewards, next states and done variables.\n",
        "        \"\"\"\n",
        "        # If we try to sample more elements that what are available from the\n",
        "        # buffer we raise an error\n",
        "        if n > len(self.buffer):\n",
        "            raise IndexError('Tried to sample too many elements from the buffer!')\n",
        "\n",
        "        # Sample without replacement the indices of the experiences\n",
        "        # np.random.choice takes 3 parameters: number of elements of the buffer,\n",
        "        # number of elements to sample and replacement.\n",
        "        indices = np.random.choice(\n",
        "            len(self.buffer),\n",
        "            size=n,\n",
        "            replace=False\n",
        "        )\n",
        "\n",
        "        # Using the indices that we just sampled build a list of chosen experiences\n",
        "        batch = [self.buffer[i] for i in indices]\n",
        "\n",
        "        # batch is a list of size n, where each element is an Experience tuple\n",
        "        # of 5 elements. To convert a list of tuples into\n",
        "        # a tuple of list we do zip(*batch). In this case this will return a\n",
        "        # tuple of 5 elements where each element is a list of n elements.\n",
        "        return zip(*batch)\n",
        "\n",
        "def running_average(x, N):\n",
        "    ''' Function used to compute the running average\n",
        "        of the last N elements of a vector x\n",
        "    '''\n",
        "    if len(x) >= N:\n",
        "        y = np.copy(x)\n",
        "        y[N-1:] = np.convolve(x, np.ones((N, )) / N, mode='valid')\n",
        "    else:\n",
        "        y = np.zeros_like(x)\n",
        "    return y\n",
        "\n",
        "def linear_decay(eps_min, eps_max, k, N_episodes):\n",
        "  Z = N_episodes*0.9\n",
        "  return max(eps_min,eps_max - ((eps_max-eps_min)*(k-1))/(Z-1))\n",
        "\n",
        "\n",
        "# Import and initialize the discrete Lunar Laner Environment\n",
        "env = gym.make('LunarLander-v2')\n",
        "env.reset()\n",
        "\n",
        "# Parameters\n",
        "N_episodes = 1000                         # Number of episodes\n",
        "discount_factor = 0.99                    # Value of the discount factor\n",
        "n_ep_running_average = 50                    # Running average of 50 episodes\n",
        "n_actions = env.action_space.n               # Number of available actions\n",
        "dim_state = len(env.observation_space.high)  # State dimensionality\n",
        "eps_max = 0.99\n",
        "eps_min = 0.05\n",
        "N = 64\n",
        "lr=0.001\n",
        "buffer_size = 20000\n",
        "target_update = round(buffer_size/N)\n",
        "# We will use these variables to compute the average episodic reward and\n",
        "# the average number of steps per episode\n",
        "episode_reward_list = []       # this list contains the total reward per episode\n",
        "episode_number_of_steps = []   # this list contains the number of steps per episode\n",
        "\n",
        "buffer = ExperienceReplayBuffer(maximum_length=buffer_size)\n",
        "#print(dim_state)\n",
        "#print(n_actions)\n",
        "# Random agent initialization\n",
        "agent_random = RandomAgent(n_actions)\n",
        "agent_DQN = DQNAgent(n_actions)\n",
        "\n",
        "main_network = DQN(dim_state, n_actions)\n",
        "target_network = DQN(dim_state, n_actions)\n",
        "target_network.load_state_dict(main_network.state_dict())\n",
        "optimizer = optim.Adam(main_network.parameters(), lr)\n",
        "\n",
        "### Training process\n",
        "\n",
        "# trange is an alternative to range in python, from the tqdm library\n",
        "# It shows a nice progression bar that you can update with useful information\n",
        "EPISODES = trange(N_episodes, desc='Episode: ', leave=True)\n",
        "\n",
        "for i in EPISODES:\n",
        "  \n",
        "    # Reset enviroment data and initialize variables\n",
        "    done = False\n",
        "    state = env.reset()\n",
        "    total_episode_reward = 0\n",
        "    t = 0\n",
        "    eps = linear_decay(eps_min, eps_max, i+1, N_episodes)\n",
        "    while not done:\n",
        "        \n",
        "        # Take epsilon-greedy action\n",
        "        if random.random() < eps:\n",
        "          action = agent_random.forward(state)\n",
        "        else:\n",
        "           state_tensor = torch.tensor([state],\n",
        "                                    requires_grad=False,\n",
        "                                    dtype=torch.float32)\n",
        "           values = main_network(state_tensor)\n",
        "           action = values.max(1)[1].item()\n",
        "\n",
        "        \n",
        "        # Get next state and reward.  The done variable\n",
        "        # will be True if you reached the goal position,\n",
        "        # False otherwise\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        exp = Experience(state, action, reward, next_state, done)\n",
        "        buffer.append(exp)\n",
        "        if len(buffer) >= buffer_size *0.3:\n",
        "\n",
        "          states, actions, rewards, next_states, dones = buffer.sample_batch(\n",
        "                N)\n",
        "          actions = torch.tensor([actions],\n",
        "                            requires_grad=False,\n",
        "                            dtype=torch.int64)\n",
        "          values = main_network(torch.tensor(states,\n",
        "                            requires_grad=True,\n",
        "                            dtype=torch.float32)).gather(1, actions)\n",
        "          next_values = target_network(torch.tensor(next_states,\n",
        "                            requires_grad=False,\n",
        "                            dtype=torch.float32))\n",
        "          rewards = torch.tensor([rewards],\n",
        "                            requires_grad=False,\n",
        "                            dtype=torch.float32)\n",
        "          target_values = rewards + discount_factor * next_values.max(1)[0]\n",
        "          \n",
        "          for j in range(len(next_values)):\n",
        "            if dones[j] == True:\n",
        "              target_values[0][j] = rewards[0][j]\n",
        "          \n",
        "          \n",
        "          \n",
        "          \n",
        "          \n",
        "\n",
        "        # Compute loss function\n",
        "          loss = nn.functional.mse_loss(\n",
        "                            values,target_values)\n",
        "\n",
        "        # Compute gradient\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "\n",
        "        # Clip gradient norm to 1\n",
        "          nn.utils.clip_grad_norm_(main_network.parameters(), max_norm=1.)\n",
        "\n",
        "        # Perform backward pass (backpropagation)\n",
        "          optimizer.step()\n",
        "      \n",
        "\n",
        "        # Update episode reward\n",
        "        total_episode_reward += reward\n",
        "\n",
        "        # Update state for next iteration\n",
        "        state = next_state\n",
        "        t+= 1\n",
        "        if  t % target_update == 0:\n",
        "            print(t)\n",
        "            target_network.load_state_dict(main_network.state_dict())\n",
        "\n",
        "    # Append episode reward and total number of steps\n",
        "    episode_reward_list.append(total_episode_reward)\n",
        "    episode_number_of_steps.append(t)\n",
        "\n",
        "    # Close environment\n",
        "    env.close()\n",
        "\n",
        "    # Updates the tqdm update bar with fresh information\n",
        "    # (episode number, total reward of the last episode, total number of Steps\n",
        "    # of the last episode, average reward, average number of steps)\n",
        "    EPISODES.set_description(\n",
        "        \"Episode {} - Reward/Steps: {:.1f}/{} - Avg. Reward/Steps: {:.1f}/{}\".format(\n",
        "        i, total_episode_reward, t,\n",
        "        running_average(episode_reward_list, n_ep_running_average)[-1],\n",
        "        running_average(episode_number_of_steps, n_ep_running_average)[-1]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Plot Rewards and steps\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 9))\n",
        "ax[0].plot([i for i in range(1, N_episodes+1)], episode_reward_list, label='Episode reward')\n",
        "ax[0].plot([i for i in range(1, N_episodes+1)], running_average(\n",
        "    episode_reward_list, n_ep_running_average), label='Avg. episode reward')\n",
        "ax[0].set_xlabel('Episodes')\n",
        "ax[0].set_ylabel('Total reward')\n",
        "ax[0].set_title('Total Reward vs Episodes')\n",
        "ax[0].legend()\n",
        "ax[0].grid(alpha=0.3)\n",
        "\n",
        "ax[1].plot([i for i in range(1, N_episodes+1)], episode_number_of_steps, label='Steps per episode')\n",
        "ax[1].plot([i for i in range(1, N_episodes+1)], running_average(\n",
        "    episode_number_of_steps, n_ep_running_average), label='Avg. number of steps per episode')\n",
        "ax[1].set_xlabel('Episodes')\n",
        "ax[1].set_ylabel('Total number of steps')\n",
        "ax[1].set_title('Total number of steps vs Episodes')\n",
        "ax[1].legend()\n",
        "ax[1].grid(alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "SreGWBxw0diS",
        "outputId": "a3d23878-dc99-4154-81cf-949bd580b99a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "Episode:   0%|          | 0/1000 [00:00<?, ?it/s]<ipython-input-2-c249c40a7fc9>:210: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
            "  state_tensor = torch.tensor([state],\n",
            "Episode 125 - Reward/Steps: -426.0/110 - Avg. Reward/Steps: -235.8/100:  13%|█▎        | 126/1000 [00:19<02:17,  6.34it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c249c40a7fc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mbuffer_size\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m           states, actions, rewards, next_states, dones = buffer.sample_batch(\n\u001b[0m\u001b[1;32m    226\u001b[0m                 N)\n\u001b[1;32m    227\u001b[0m           actions = torch.tensor([actions],\n",
            "\u001b[0;32m<ipython-input-2-c249c40a7fc9>\u001b[0m in \u001b[0;36msample_batch\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# np.random.choice takes 3 parameters: number of elements of the buffer,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# number of elements to sample and replacement.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         indices = np.random.choice(\n\u001b[0m\u001b[1;32m    127\u001b[0m             \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}