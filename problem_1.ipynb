{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXrcN9jqF6lkL1zOphFCII",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeye-cyber/DL-project/blob/master/problem_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kA0POWp7vAZr",
        "outputId": "a211afed-2a57-4756-83f6-9dd94a55901b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.8/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (4.13.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (1.5.0)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (4.1.1)\n",
            "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (2.1.0)\n",
            "Collecting box2d-py==2.3.5\n",
            "  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym[box2d]) (3.11.0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp38-cp38-linux_x86_64.whl size=2113780 sha256=a084ada830a0014804e06b3e242a8f9206dd035408c6f6cd41c90ea92879e652\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/95/16/1dc99ff9a3f316ff245fdb5c9086cd13c35dad630809909075\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.5\n"
          ]
        }
      ],
      "source": [
        "!pip3 install gym[box2d]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch.nn.modules.linear import Linear\n",
        "from torch.nn.modules.activation import ReLU\n",
        "import numpy as np\n",
        "import gym\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import trange\n",
        "from collections import deque, namedtuple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "class Agent(object):\n",
        "    ''' Base agent class, used as a parent class\n",
        "\n",
        "        Args:\n",
        "            n_actions (int): number of actions\n",
        "\n",
        "        Attributes:\n",
        "            n_actions (int): where we store the number of actions\n",
        "            last_action (int): last action taken by the agent\n",
        "    '''\n",
        "    def __init__(self, n_actions: int):\n",
        "        self.n_actions = n_actions\n",
        "        self.last_action = None\n",
        "\n",
        "    def forward(self, state: np.ndarray):\n",
        "        ''' Performs a forward computation '''\n",
        "        pass\n",
        "\n",
        "    def backward(self):\n",
        "        ''' Performs a backward pass on the network '''\n",
        "        pass\n",
        "\n",
        "\n",
        "class RandomAgent(Agent):\n",
        "    ''' Agent taking actions uniformly at random, child of the class Agent'''\n",
        "    def __init__(self, n_actions: int):\n",
        "        super(RandomAgent, self).__init__(n_actions)\n",
        "\n",
        "    def forward(self, state: np.ndarray) -> int:\n",
        "        ''' Compute an action uniformly at random across n_actions possible\n",
        "            choices\n",
        "\n",
        "            Returns:\n",
        "                action (int): the random action\n",
        "        '''\n",
        "        self.last_action = np.random.randint(0, self.n_actions)\n",
        "        return self.last_action\n",
        "\n",
        "\n",
        "class DQN(nn.Module):\n",
        "      def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, output_size)\n",
        "        )\n",
        "      def forward(self, state):\n",
        "        ''' Performs a forward computation '''\n",
        "        return self.network(state)\n",
        "        \n",
        "   \n",
        "class DQNAgent(object):\n",
        "    ''' Base agent class, used as a parent class\n",
        "\n",
        "        Args:\n",
        "            n_actions (int): number of actions\n",
        "\n",
        "        Attributes:\n",
        "            n_actions (int): where we store the number of actions\n",
        "            last_action (int): last action taken by the agent\n",
        "    '''\n",
        "    def __init__(self, n_actions: int):\n",
        "        self.n_actions = n_actions\n",
        "        self.last_action = None\n",
        "\n",
        "    def forward(self, state: np.ndarray):\n",
        "        ''' Performs a forward computation '''\n",
        "        pass\n",
        "\n",
        "    def backward(self):\n",
        "        ''' Performs a backward pass on the network '''\n",
        "        pass\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "Experience = namedtuple('Experience',\n",
        "                        ['state', 'action', 'reward', 'next_state', 'done'])\n",
        "\n",
        "class ExperienceReplayBuffer(object):\n",
        "    \"\"\" Class used to store a buffer containing experiences of the RL agent.\n",
        "    \"\"\"\n",
        "    def __init__(self, maximum_length):\n",
        "        # Create buffer of maximum length\n",
        "        self.buffer = deque(maxlen=maximum_length)\n",
        "\n",
        "    def append(self, experience):\n",
        "        # Append experience to the buffer\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def __len__(self):\n",
        "        # overload len operator\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def sample_batch(self, n):\n",
        "        \"\"\" Function used to sample experiences from the buffer.\n",
        "            returns 5 lists, each of size n. Returns a list of state, actions,\n",
        "            rewards, next states and done variables.\n",
        "        \"\"\"\n",
        "        # If we try to sample more elements that what are available from the\n",
        "        # buffer we raise an error\n",
        "        if n > len(self.buffer):\n",
        "            raise IndexError('Tried to sample too many elements from the buffer!')\n",
        "\n",
        "        # Sample without replacement the indices of the experiences\n",
        "        # np.random.choice takes 3 parameters: number of elements of the buffer,\n",
        "        # number of elements to sample and replacement.\n",
        "        indices = np.random.choice(\n",
        "            len(self.buffer),\n",
        "            size=n,\n",
        "            replace=False\n",
        "        )\n",
        "\n",
        "        # Using the indices that we just sampled build a list of chosen experiences\n",
        "        batch = [self.buffer[i] for i in indices]\n",
        "\n",
        "        # batch is a list of size n, where each element is an Experience tuple\n",
        "        # of 5 elements. To convert a list of tuples into\n",
        "        # a tuple of list we do zip(*batch). In this case this will return a\n",
        "        # tuple of 5 elements where each element is a list of n elements.\n",
        "        return zip(*batch)\n",
        "\n",
        "def running_average(x, N):\n",
        "    ''' Function used to compute the running average\n",
        "        of the last N elements of a vector x\n",
        "    '''\n",
        "    if len(x) >= N:\n",
        "        y = np.copy(x)\n",
        "        y[N-1:] = np.convolve(x, np.ones((N, )) / N, mode='valid')\n",
        "    else:\n",
        "        y = np.zeros_like(x)\n",
        "    return y\n",
        "\n",
        "def linear_decay(eps_min, eps_max, k, N_episodes):\n",
        "  Z = N_episodes*0.9\n",
        "  return max(eps_min,eps_max - ((eps_max-eps_min)*(k-1))/(Z-1))\n",
        "\n",
        "\n",
        "# Import and initialize the discrete Lunar Laner Environment\n",
        "env = gym.make('LunarLander-v2')\n",
        "env.reset()\n",
        "\n",
        "# Parameters\n",
        "N_episodes = 500                        # Number of episodes\n",
        "discount_factor = 0.99                    # Value of the discount factor\n",
        "n_ep_running_average = 50                    # Running average of 50 episodes\n",
        "n_actions = env.action_space.n               # Number of available actions\n",
        "dim_state = len(env.observation_space.high)  # State dimensionality\n",
        "eps_max = 0.99\n",
        "eps_min = 0.05\n",
        "N = 64\n",
        "lr=0.0001\n",
        "buffer_size = 15000\n",
        "target_update = round(buffer_size/N)\n",
        "# We will use these variables to compute the average episodic reward and\n",
        "# the average number of steps per episode\n",
        "episode_reward_list = []       # this list contains the total reward per episode\n",
        "episode_number_of_steps = []   # this list contains the number of steps per episode\n",
        "\n",
        "buffer = ExperienceReplayBuffer(maximum_length=buffer_size)\n",
        "#print(dim_state)\n",
        "#print(n_actions)\n",
        "# Random agent initialization\n",
        "agent_random = RandomAgent(n_actions)\n",
        "agent_DQN = DQNAgent(n_actions)\n",
        "\n",
        "main_network = DQN(dim_state, n_actions)\n",
        "target_network = DQN(dim_state, n_actions)\n",
        "target_network.load_state_dict(main_network.state_dict())\n",
        "optimizer = optim.Adam(main_network.parameters(), lr)\n",
        "\n",
        "### Training process\n",
        "\n",
        "# trange is an alternative to range in python, from the tqdm library\n",
        "# It shows a nice progression bar that you can update with useful information\n",
        "EPISODES = trange(N_episodes, desc='Episode: ', leave=True)\n",
        "\n",
        "for i in EPISODES:\n",
        "  \n",
        "    # Reset enviroment data and initialize variables\n",
        "    done = False\n",
        "    state = env.reset()\n",
        "    total_episode_reward = 0\n",
        "    t = 0\n",
        "    eps = linear_decay(eps_min, eps_max, i+1, N_episodes)\n",
        "    while not done:\n",
        "        \n",
        "        # Take epsilon-greedy action\n",
        "        if random.random() < eps:\n",
        "          action = agent_random.forward(state)\n",
        "        else:\n",
        "           state_tensor = torch.tensor([state],\n",
        "                                    requires_grad=False,\n",
        "                                    dtype=torch.float32)\n",
        "           values = main_network(state_tensor)\n",
        "           action = values.max(1)[1].item()\n",
        "\n",
        "        \n",
        "        # Get next state and reward.  The done variable\n",
        "        # will be True if you reached the goal position,\n",
        "        # False otherwise\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        exp = Experience(state, action, reward, next_state, done)\n",
        "        buffer.append(exp)\n",
        "        if len(buffer) >= buffer_size *0.3:\n",
        "\n",
        "          states, actions, rewards, next_states, dones = buffer.sample_batch(\n",
        "                N)\n",
        "          actions = torch.tensor(actions,\n",
        "                            requires_grad=False)\n",
        "          values = main_network(torch.tensor(states,\n",
        "                            requires_grad=True,\n",
        "                            dtype=torch.float32)).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
        "          next_values = target_network(torch.tensor(next_states,\n",
        "                            requires_grad=False,\n",
        "                            dtype=torch.float32))\n",
        "          rewards = torch.tensor([rewards],\n",
        "                            requires_grad=False,\n",
        "                            dtype=torch.float32)\n",
        "          target_values = rewards + discount_factor * next_values.max(1)[0]\n",
        "          \n",
        "          for j in range(len(next_values)):\n",
        "            if dones[j] == True:\n",
        "              target_values[0][j] = rewards[0][j]\n",
        "          \n",
        "          \n",
        "          \n",
        "          \n",
        "          \n",
        "\n",
        "        # Compute loss function\n",
        "          loss = nn.functional.mse_loss(\n",
        "                            values,target_values)\n",
        "\n",
        "        # Compute gradient\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "\n",
        "        # Clip gradient norm to 1\n",
        "          nn.utils.clip_grad_norm_(main_network.parameters(), max_norm=1.)\n",
        "\n",
        "        # Perform backward pass (backpropagation)\n",
        "          optimizer.step()\n",
        "      \n",
        "\n",
        "        # Update episode reward\n",
        "        total_episode_reward += reward\n",
        "\n",
        "        # Update state for next iteration\n",
        "        state = next_state\n",
        "        t+= 1\n",
        "        if  t % target_update == 0:\n",
        "            print(t)\n",
        "            target_network.load_state_dict(main_network.state_dict())\n",
        "\n",
        "    # Append episode reward and total number of steps\n",
        "    episode_reward_list.append(total_episode_reward)\n",
        "    episode_number_of_steps.append(t)\n",
        "\n",
        "    # Close environment\n",
        "    env.close()\n",
        "\n",
        "    # Updates the tqdm update bar with fresh information\n",
        "    # (episode number, total reward of the last episode, total number of Steps\n",
        "    # of the last episode, average reward, average number of steps)\n",
        "    EPISODES.set_description(\n",
        "        \"Episode {} - Reward/Steps: {:.1f}/{} - Avg. Reward/Steps: {:.1f}/{}\".format(\n",
        "        i, total_episode_reward, t,\n",
        "        running_average(episode_reward_list, n_ep_running_average)[-1],\n",
        "        running_average(episode_number_of_steps, n_ep_running_average)[-1]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Plot Rewards and steps\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 9))\n",
        "ax[0].plot([i for i in range(1, N_episodes+1)], episode_reward_list, label='Episode reward')\n",
        "ax[0].plot([i for i in range(1, N_episodes+1)], running_average(\n",
        "    episode_reward_list, n_ep_running_average), label='Avg. episode reward')\n",
        "ax[0].set_xlabel('Episodes')\n",
        "ax[0].set_ylabel('Total reward')\n",
        "ax[0].set_title('Total Reward vs Episodes')\n",
        "ax[0].legend()\n",
        "ax[0].grid(alpha=0.3)\n",
        "\n",
        "ax[1].plot([i for i in range(1, N_episodes+1)], episode_number_of_steps, label='Steps per episode')\n",
        "ax[1].plot([i for i in range(1, N_episodes+1)], running_average(\n",
        "    episode_number_of_steps, n_ep_running_average), label='Avg. number of steps per episode')\n",
        "ax[1].set_xlabel('Episodes')\n",
        "ax[1].set_ylabel('Total number of steps')\n",
        "ax[1].set_title('Total number of steps vs Episodes')\n",
        "ax[1].legend()\n",
        "ax[1].grid(alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SreGWBxw0diS",
        "outputId": "0ee55c85-00e4-42f4-953a-1fcba60ec5c6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 48 - Reward/Steps: -210.1/73 - Avg. Reward/Steps: 0.0/0:   9%|▉         | 45/500 [00:01<00:11, 39.54it/s]<ipython-input-9-c2158a358902>:250: UserWarning: Using a target size (torch.Size([1, 64])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  loss = nn.functional.mse_loss(\n",
            "Episode 134 - Reward/Steps: -166.9/275 - Avg. Reward/Steps: -160.1/114:  27%|██▋       | 135/500 [00:38<03:13,  1.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 176 - Reward/Steps: -20.0/133 - Avg. Reward/Steps: -125.2/137:  35%|███▌      | 177/500 [00:59<02:47,  1.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 177 - Reward/Steps: -110.2/531 - Avg. Reward/Steps: -126.7/145:  36%|███▌      | 178/500 [01:01<05:22,  1.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 186 - Reward/Steps: -129.2/253 - Avg. Reward/Steps: -126.2/151:  37%|███▋      | 187/500 [01:06<03:22,  1.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 205 - Reward/Steps: -125.4/282 - Avg. Reward/Steps: -103.0/155:  41%|████      | 206/500 [01:16<03:18,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 215 - Reward/Steps: -58.0/273 - Avg. Reward/Steps: -94.5/164:  43%|████▎     | 216/500 [01:22<03:42,  1.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 230 - Reward/Steps: -80.6/254 - Avg. Reward/Steps: -87.4/157:  46%|████▌     | 231/500 [01:30<02:38,  1.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 232 - Reward/Steps: -74.9/149 - Avg. Reward/Steps: -83.3/158:  47%|████▋     | 233/500 [01:31<02:25,  1.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 233 - Reward/Steps: -24.6/1000 - Avg. Reward/Steps: -82.5/175:  47%|████▋     | 234/500 [01:35<07:10,  1.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 239 - Reward/Steps: -95.2/241 - Avg. Reward/Steps: -73.0/193:  48%|████▊     | 240/500 [01:42<03:49,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 246 - Reward/Steps: -42.1/188 - Avg. Reward/Steps: -66.8/212:  49%|████▉     | 247/500 [01:49<02:50,  1.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 249 - Reward/Steps: -44.9/224 - Avg. Reward/Steps: -64.5/231:  50%|█████     | 250/500 [01:55<05:01,  1.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 253 - Reward/Steps: -51.7/154 - Avg. Reward/Steps: -57.0/250:  51%|█████     | 254/500 [02:01<04:41,  1.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 255 - Reward/Steps: -14.7/218 - Avg. Reward/Steps: -51.9/264:  51%|█████     | 256/500 [02:06<06:41,  1.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 256 - Reward/Steps: 85.0/1000 - Avg. Reward/Steps: -48.3/281:  51%|█████▏    | 257/500 [02:11<10:15,  2.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 258 - Reward/Steps: -54.8/198 - Avg. Reward/Steps: -50.5/288:  52%|█████▏    | 259/500 [02:13<07:17,  1.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 259 - Reward/Steps: 54.2/1000 - Avg. Reward/Steps: -47.4/306:  52%|█████▏    | 260/500 [02:17<10:06,  2.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 260 - Reward/Steps: -19.9/468 - Avg. Reward/Steps: -44.7/311:  52%|█████▏    | 261/500 [02:19<09:18,  2.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "468\n",
            "234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 261 - Reward/Steps: -26.2/378 - Avg. Reward/Steps: -42.9/315:  52%|█████▏    | 262/500 [02:21<08:15,  2.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 262 - Reward/Steps: 29.7/1000 - Avg. Reward/Steps: -40.4/332:  53%|█████▎    | 263/500 [02:25<11:12,  2.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 264 - Reward/Steps: -68.1/99 - Avg. Reward/Steps: -36.3/345:  53%|█████▎    | 265/500 [02:31<10:24,  2.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 265 - Reward/Steps: -19.7/390 - Avg. Reward/Steps: -35.5/348:  53%|█████▎    | 266/500 [02:33<09:25,  2.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 267 - Reward/Steps: 18.5/227 - Avg. Reward/Steps: -29.6/366:  54%|█████▎    | 268/500 [02:38<08:51,  2.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 270 - Reward/Steps: 2.2/233 - Avg. Reward/Steps: -26.5/385:  54%|█████▍    | 271/500 [02:44<06:57,  1.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 271 - Reward/Steps: 40.3/1000 - Avg. Reward/Steps: -22.1/402:  54%|█████▍    | 272/500 [02:48<09:50,  2.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 272 - Reward/Steps: 79.4/1000 - Avg. Reward/Steps: -20.5/418:  55%|█████▍    | 273/500 [02:53<12:18,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 276 - Reward/Steps: -1.9/236 - Avg. Reward/Steps: -19.1/438:  55%|█████▌    | 277/500 [03:00<06:21,  1.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 277 - Reward/Steps: -104.3/276 - Avg. Reward/Steps: -20.4/440:  56%|█████▌    | 278/500 [03:01<05:33,  1.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "234\n",
            "468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 278 - Reward/Steps: -69.6/625 - Avg. Reward/Steps: -20.1/449:  56%|█████▌    | 279/500 [03:03<06:42,  1.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 282 - Reward/Steps: -187.7/223 - Avg. Reward/Steps: -19.7/468:  57%|█████▋    | 283/500 [03:10<04:47,  1.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 283 - Reward/Steps: 95.2/1000 - Avg. Reward/Steps: -17.3/468:  57%|█████▋    | 284/500 [03:14<08:14,  2.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 284 - Reward/Steps: 10.2/1000 - Avg. Reward/Steps: -18.9/468:  57%|█████▋    | 285/500 [03:19<10:40,  2.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 286 - Reward/Steps: 18.7/183 - Avg. Reward/Steps: -17.5/475:  57%|█████▋    | 287/500 [03:21<07:06,  2.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 287 - Reward/Steps: 58.3/1000 - Avg. Reward/Steps: -15.9/492:  58%|█████▊    | 288/500 [03:26<09:50,  2.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 288 - Reward/Steps: 145.1/1000 - Avg. Reward/Steps: -10.5/509:  58%|█████▊    | 289/500 [03:30<11:22,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 289 - Reward/Steps: 107.1/1000 - Avg. Reward/Steps: -6.4/525:  58%|█████▊    | 290/500 [03:34<12:45,  3.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 290 - Reward/Steps: -29.7/453 - Avg. Reward/Steps: -7.5/514:  58%|█████▊    | 291/500 [03:36<10:50,  3.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 291 - Reward/Steps: 153.0/1000 - Avg. Reward/Steps: -3.5/531:  58%|█████▊    | 292/500 [03:41<12:19,  3.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 292 - Reward/Steps: 3.8/1000 - Avg. Reward/Steps: -2.8/547:  59%|█████▊    | 293/500 [03:45<13:15,  3.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 294 - Reward/Steps: -28.1/218 - Avg. Reward/Steps: 1.4/566:  59%|█████▉    | 295/500 [03:51<10:27,  3.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 295 - Reward/Steps: 152.9/891 - Avg. Reward/Steps: 4.9/581:  59%|█████▉    | 296/500 [03:55<12:00,  3.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 296 - Reward/Steps: 77.4/1000 - Avg. Reward/Steps: 7.3/598:  59%|█████▉    | 297/500 [04:00<13:05,  3.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 299 - Reward/Steps: -9.5/168 - Avg. Reward/Steps: 7.8/597:  60%|██████    | 300/500 [04:05<07:31,  2.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 300 - Reward/Steps: 128.6/1000 - Avg. Reward/Steps: 8.4/597:  60%|██████    | 301/500 [04:10<09:30,  2.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 301 - Reward/Steps: 18.7/1000 - Avg. Reward/Steps: 8.5/613:  60%|██████    | 302/500 [04:14<10:58,  3.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 302 - Reward/Steps: 84.3/1000 - Avg. Reward/Steps: 10.4/630:  61%|██████    | 303/500 [04:18<11:52,  3.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 303 - Reward/Steps: 108.3/1000 - Avg. Reward/Steps: 13.6/647:  61%|██████    | 304/500 [04:22<12:14,  3.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 304 - Reward/Steps: 140.6/1000 - Avg. Reward/Steps: 15.4/647:  61%|██████    | 305/500 [04:27<12:31,  3.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 305 - Reward/Steps: 93.6/1000 - Avg. Reward/Steps: 17.6/663:  61%|██████    | 306/500 [04:31<13:15,  4.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 308 - Reward/Steps: -36.9/119 - Avg. Reward/Steps: 21.6/655:  62%|██████▏   | 309/500 [04:36<06:59,  2.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 309 - Reward/Steps: 115.8/1000 - Avg. Reward/Steps: 22.8/655:  62%|██████▏   | 310/500 [04:41<09:35,  3.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 311 - Reward/Steps: -20.3/130 - Avg. Reward/Steps: 23.5/661:  62%|██████▏   | 312/500 [04:47<08:35,  2.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 312 - Reward/Steps: -77.0/762 - Avg. Reward/Steps: 21.3/656:  63%|██████▎   | 313/500 [04:50<09:17,  2.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 313 - Reward/Steps: 49.5/1000 - Avg. Reward/Steps: 20.8/656:  63%|██████▎   | 314/500 [04:56<11:30,  3.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 314 - Reward/Steps: 109.4/1000 - Avg. Reward/Steps: 24.3/674:  63%|██████▎   | 315/500 [05:00<12:20,  4.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 316 - Reward/Steps: -19.8/133 - Avg. Reward/Steps: 23.6/669:  63%|██████▎   | 317/500 [05:06<09:36,  3.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 318 - Reward/Steps: -25.5/216 - Avg. Reward/Steps: 23.5/669:  64%|██████▍   | 319/500 [05:11<07:51,  2.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 319 - Reward/Steps: 136.1/1000 - Avg. Reward/Steps: 29.9/685:  64%|██████▍   | 320/500 [05:14<08:45,  2.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 320 - Reward/Steps: 48.4/1000 - Avg. Reward/Steps: 30.9/701:  64%|██████▍   | 321/500 [05:20<10:58,  3.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 321 - Reward/Steps: 119.8/1000 - Avg. Reward/Steps: 32.5/701:  64%|██████▍   | 322/500 [05:25<12:07,  4.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 322 - Reward/Steps: 112.7/1000 - Avg. Reward/Steps: 33.1/701:  65%|██████▍   | 323/500 [05:29<12:29,  4.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 323 - Reward/Steps: 65.3/1000 - Avg. Reward/Steps: 36.1/701:  65%|██████▍   | 324/500 [05:34<12:38,  4.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 325 - Reward/Steps: -27.6/107 - Avg. Reward/Steps: 39.1/717:  65%|██████▌   | 326/500 [05:39<09:12,  3.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 326 - Reward/Steps: -10.6/1000 - Avg. Reward/Steps: 38.9/732:  65%|██████▌   | 327/500 [05:44<10:51,  3.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 327 - Reward/Steps: 23.3/1000 - Avg. Reward/Steps: 41.5/747:  66%|██████▌   | 328/500 [05:48<11:27,  4.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 328 - Reward/Steps: 63.5/1000 - Avg. Reward/Steps: 44.1/754:  66%|██████▌   | 329/500 [05:53<11:35,  4.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 330 - Reward/Steps: 55.5/180 - Avg. Reward/Steps: 44.9/742:  66%|██████▌   | 331/500 [05:55<07:00,  2.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 331 - Reward/Steps: 208.4/582 - Avg. Reward/Steps: 49.7/750:  66%|██████▋   | 332/500 [05:57<06:58,  2.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 332 - Reward/Steps: 74.1/1000 - Avg. Reward/Steps: 55.0/766:  67%|██████▋   | 333/500 [06:02<08:31,  3.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 333 - Reward/Steps: 31.8/1000 - Avg. Reward/Steps: 53.7/766:  67%|██████▋   | 334/500 [06:06<09:48,  3.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 334 - Reward/Steps: 131.6/1000 - Avg. Reward/Steps: 56.1/766:  67%|██████▋   | 335/500 [06:10<10:06,  3.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 335 - Reward/Steps: 83.8/1000 - Avg. Reward/Steps: 58.6/777:  67%|██████▋   | 336/500 [06:14<10:16,  3.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 336 - Reward/Steps: 243.9/940 - Avg. Reward/Steps: 63.1/792:  67%|██████▋   | 337/500 [06:18<10:28,  3.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 338 - Reward/Steps: 20.1/211 - Avg. Reward/Steps: 55.4/761:  68%|██████▊   | 339/500 [06:20<06:05,  2.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 339 - Reward/Steps: 187.3/634 - Avg. Reward/Steps: 57.0/754:  68%|██████▊   | 340/500 [06:23<06:32,  2.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 340 - Reward/Steps: 0.5/1000 - Avg. Reward/Steps: 57.6/765:  68%|██████▊   | 341/500 [06:27<08:12,  3.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 341 - Reward/Steps: 42.9/1000 - Avg. Reward/Steps: 55.4/765:  68%|██████▊   | 342/500 [06:33<10:04,  3.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 342 - Reward/Steps: 122.6/1000 - Avg. Reward/Steps: 57.8/765:  69%|██████▊   | 343/500 [06:37<10:33,  4.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 343 - Reward/Steps: 135.0/1000 - Avg. Reward/Steps: 58.7/765:  69%|██████▉   | 344/500 [06:41<10:24,  4.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 344 - Reward/Steps: -43.1/1000 - Avg. Reward/Steps: 58.4/780:  69%|██████▉   | 345/500 [06:47<11:26,  4.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 345 - Reward/Steps: 13.3/1000 - Avg. Reward/Steps: 55.6/783:  69%|██████▉   | 346/500 [06:52<11:37,  4.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 346 - Reward/Steps: 82.9/1000 - Avg. Reward/Steps: 55.7/783:  69%|██████▉   | 347/500 [06:56<11:37,  4.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 347 - Reward/Steps: 45.0/1000 - Avg. Reward/Steps: 55.3/783:  70%|██████▉   | 348/500 [07:01<11:54,  4.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 348 - Reward/Steps: -58.3/1000 - Avg. Reward/Steps: 55.4/799:  70%|██████▉   | 349/500 [07:06<11:59,  4.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 350 - Reward/Steps: -25.6/182 - Avg. Reward/Steps: 55.0/799:  70%|███████   | 351/500 [07:11<08:27,  3.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 351 - Reward/Steps: 62.5/1000 - Avg. Reward/Steps: 55.9/799:  70%|███████   | 352/500 [07:15<09:12,  3.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 353 - Reward/Steps: 301.9/272 - Avg. Reward/Steps: 60.9/784:  71%|███████   | 354/500 [07:21<07:18,  3.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "234\n",
            "468\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 356 - Reward/Steps: 47.0/122 - Avg. Reward/Steps: 56.3/750:  71%|███████▏  | 357/500 [07:27<04:46,  2.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 359 - Reward/Steps: 31.7/135 - Avg. Reward/Steps: 46.4/750:  72%|███████▏  | 360/500 [07:32<03:36,  1.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 361 - Reward/Steps: -1.6/198 - Avg. Reward/Steps: 48.3/751:  72%|███████▏  | 362/500 [07:37<04:30,  1.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n",
            "702\n",
            "936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 362 - Reward/Steps: 150.4/1000 - Avg. Reward/Steps: 52.9/756:  73%|███████▎  | 363/500 [07:41<05:52,  2.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 366 - Reward/Steps: -53.1/130 - Avg. Reward/Steps: 51.0/718:  73%|███████▎  | 367/500 [07:46<02:53,  1.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpisode 366 - Reward/Steps: -53.1/130 - Avg. Reward/Steps: 51.0/718:  73%|███████▎  | 367/500 [07:49<02:50,  1.28s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-c2158a358902>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;31m# Compute gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m           \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0mper_device_and_dtype_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_zero_grad_profile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}